{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning\n",
    "For this particular classification problem, I decided use multiple machine learning algorithms that utilizes ensemble learning. Below are the following machine learning algorithms that were used to predict whether restaurant will fail or remain open with the given features.\n",
    "\n",
    "1. **Random Forest** (Bagging)\n",
    "2. **AdaBoost** (use of increasing the weight of misclassified data points)\n",
    "3. **Gradient Boosting** (learning previous mistakes with residual error)\n",
    "\n",
    "Based on the result from those three machine learning algorithms, I'll be determining best machine learning algorithm based on accurarcy and computational time. All algorithms were analyzed by initially using default parameter and making improvements by optimizing model parameters using either RandomizedSearchCV and GridSearchCV.\n",
    "\n",
    "As standard, features were tested using standard train/test split ratio of 7:3. 70% were fitted to model and 30% were left for testing to evaluate machine learning algorithms. The baseline was having at least 100 decision trees (or stumps for AdaBoost and Gradient Boosting).\n",
    "\n",
    "### 6.1 Random Forest (Bagging Approach)\n",
    "My focus is to prevent or minimize overfitting or having high false positives in the result; therefore, random forest is used to check since it minimizes overfitting and handles large dataset with high dimensionality\n",
    "\n",
    "#### Default approach without hyperparameter tuning\n",
    "With the initial baseline result with 100 decision trees (n_estimators) it yielded the following result below:\n",
    "\n",
    "##### Initial Precision and Recall\n",
    "\n",
    "<img src=\"img/initial_rf_result_img.png\" alt=\"initial_rf_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "According to initial confusion matrix report, random forest classifier is better at identifying open restaurants compared to close businesses. **42% being false positive** and **8% being false negative**. Since this capstone project is about whether to lend money to the restaurant or whether aspiring restauranteur should open a restaurant. I need to adjust machine learning algorithm to focus on reducing false positives because we don't want to lend or invest in business that will eventually close.\n",
    "\n",
    "### Selecting the best tuning parameters (aka 'hyperparameters') for Random Forest \n",
    "Randomized search cv. Randomized search cv is used for Random Forest due to random forest having many parameters which may take a lot of computational time in finding best parameters without overfitting. Below are the parameters I'll be tuning:\n",
    "\n",
    "- n_estimators = number of trees in the forest\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node\n",
    "- bootstrap = method for sampling data points (with or without replacement)\n",
    "\n",
    "Randomized Search CV yielded the best parameters:\n",
    "- n_estimators: 1800\n",
    "- min_samples_split: 2\n",
    "- min_samples_leaf: 1\n",
    "- max_features: 'auto'\n",
    "- max_depth: 50\n",
    "- criterion: entropy\n",
    "- bootstrap: FAlse\n",
    "\n",
    "#### Test result\n",
    "<img src=\"img/rf_result_img.png\" alt=\"rf_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "#### Feature Importance\n",
    "<img src=\"img/rf_feat_importance_img.png\" alt=\"rf_feat_importance\" style=\"width: 40%;\"/>\n",
    "\n",
    "#### RandomForest Summary\n",
    "\n",
    "Randomized Search CV improved its overall accuracy by 1% by improving in identifying true negatives but it did not significantly reduce false positives. Random Forest gave equal weights to all created decision trees which resulted in about 80% accuracy; we will identify whether giving different weights to each decision trees will give better result by using Adaptive Boosting.\n",
    "\n",
    "Lifespan, pos, and star rating, and sentiment related features had highest signal in determining whether restaurant will fail or not. With strong emphais on lifespan feature. We will later identify whether this is true for other two algorithms we will see in later section.\n",
    "\n",
    "### 6.2 AdaBoost (Sampling Distribution)\n",
    "I decided to use AdaBoost algorithm to inspect how well it performs when we focus on what the model misclassifies through **sampling distribution**. In other words, we give more focus on data points that makes a mistake so that it learns from its mistake when creating n-stumps.\n",
    "\n",
    "#### Default approach without hyperparameter tuning\n",
    "Inspecting baseline when AdaBoost is used without tuning with the exception n_estimators (how many stumps we like to create).\n",
    "\n",
    "#### Initial Recall and Precision\n",
    "<img src=\"img/initial_ada_result_img.png\" alt=\"initial_ada_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "AdaBoost algorithm took **5.45 s** in creating and evaluating 100 decision trees with **76%** accuracy without hyperparameter tuning. Overall AdaBoost provided less desirable result compared to random forest classifier since AdaBoost had higher false negative at **14%**. However, it did provide lower false positives at **40.5%**, decrease from **42%**.\n",
    "\n",
    "### Selecting the best tuning parameters for AdaBoost \n",
    "Will be using GridSearchCV for optimization by tuning the following parameters below:\n",
    "\n",
    "- **n_estimators**: maximum number of estimators (stumps at which boosting is terminated\n",
    "- **learning_rate**: rate at which we are adjusting the weights of our model with respect to the loss gradient\n",
    "\n",
    "#### Test result\n",
    "<img src=\"img/rf_result_img.png\" alt=\"rf_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "#### Feature Importance\n",
    "<img src=\"img/ada_feats_img.png\" alt=\"ada_feats\" style=\"width: 40%;\"/>\n",
    "\n",
    "AdaBoost's feature importance is slightly different with last 30 day review count and several other restaurant attributes turned out to be more important in identifying closed and open restaurants. Lifespan, revenue, and sentiment score related features are consistently ranked high in determining our restaurant's business status.\n",
    "\n",
    "### AdaBoost Summary\n",
    "Initial AdaBoost algorithm with default setting took **5.45 s** in creating and evaluating 100 decision trees with **76%** accuracy which is lower than Random Forest's initial accuracy. However it did provide lower false positives at **40.5%**, decrease from **42%**. I tuned its hyperparameters using GridSearchCV adjusting **n_estimators at 200** with **learning_rate of 0.5**.\n",
    "\n",
    "It yielded better overall result at **77%** getting better result at obtaining higher true negatives but at the expense of gaining more false positives.\n",
    "\n",
    "## 6.3  Gradient Boosting\n",
    "AdaBoost provided fairly okay result in identifying closed and open restaurants through sampling distribution and giving each stump its own weight in deciding which model worked best. I decided to use gradient boosting to see if it provides similar or better result to Random Forest (which currently holds best accuracy) through through residual error directly instead of giving weights to each data points.\n",
    "\n",
    "#### Initial Recall and Precision\n",
    "<img src=\"img/initial_grad_result_img.png\" alt=\"initial_grad_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "Gradient boosting did better at predicting open and closed restaurants than AdaBoost but did poorer than Random Forest, similar to all algorithms - It did not do very good job at predicting closed restaurants - instead it misclassified closed restaurants as open. It has the highest false positives (**43%**) compared to other algorithms. Overall it has initial accuracy at **78%**.\n",
    "\n",
    "### Selecting the best tuning parameters for Gradient Boosting\n",
    "Same as AdaBoost, I will be using GridSearchCV for optimization by tuning the following parameters below:\n",
    "\n",
    "- **n_estimators**: maximum number of estimators (stumps at which boosting is terminated\n",
    "- **learning_rate**: rate at which we are adjusting the weights of our model with respect to the loss gradient\n",
    "\n",
    "#### Test result\n",
    "<img src=\"img/grad_result_img.png\" alt=\"grad_result\" style=\"width: 40%;\"/>\n",
    "\n",
    "#### Feature Importance\n",
    "<img src=\"img/ada_feats_img.png\" alt=\"ada_feats\" style=\"width: 40%;\"/>\n",
    "\n",
    "### Gradient Boosting Summary\n",
    "Initial GradientBoosting algorithm with default setting took **9.24 s** in creating and evaluating 100 decision trees with **78%** accuracy which is lower than Random Forest's initial accuracy but higher than AdaBoost's accurracy. However it did provided highest false positives at **43%**. I tuned its hyperparameters using GridSearchCV adjusting **n_estimators at 200** with **learning_rate of 0.2**.\n",
    "\n",
    "It yielded best overall accuracy result at **79.9%** and getting least false positives at **36.9%**. So far, I would recommend using gradient boosting as it has lower chance of lending or investing on businesses that is going to fail.\n",
    "\n",
    "## 6.4 ROC and AUC\n",
    "Computing AUROC and ROC curve values and plotting for visualization purpose.\n",
    "\n",
    "<img src=\"img/auroc_result_img.png\" alt=\"auroc_result\" style=\"width: 40%;\"/>\n",
    "<img src=\"img/roc_plot_img.png\" alt=\"roc_plot\" style=\"width: 40%;\"/>\n",
    "\n",
    "### Summary\n",
    "Random forest and Gradient Boosting algorithm gave the best outcome in predicting whether restaurants are open or closed based on given features. Each algorithms had differing feature importance but few features repeatedly came into view such as lifespan, sentiment score, star rating, revenue, and review count. Those five features played important role in determining whether restaurant will strive or fail in hospitality industry with precision of **84%** and overall F1-score of **80%**.\n",
    "\n",
    "## 7. Suggested Improvement\n",
    "There are several ways to improve performances in identifying restaurant's business status using multiple data sources.\n",
    "\n",
    "- Using population demographics and income level to gauge if it impacts restaurant's price range or its revenues.\n",
    "- Comparing nearby similar competitors - whether one similar restaurant's performance affects nearby restaurant's performance.\n",
    "- Using actual restaurant's revenue data instead of using speculative revenues which were calculated using price and review_count columns. \n",
    "\n",
    "## 8. Project Summary\n",
    "- The model was built for restaurant lending purposes that helps decides whether to invest in a independent restaurant or not based on its given data.\n",
    "- Yelp dataset was used and analyzed to build classification model that correctly identifies restaurant's business status.\n",
    "- Four major predictive features were identified which is lifespan, sentiment analysis scores, star ratings, and revenues.\n",
    "- Used three ML algorithms based on bagging and boosting (Random Forest, AdaBoost, and Gradient Boosting) which yielded highest precision score of **79%** and recall score of **90%** with overall F1-score of **80%**.\n",
    "\n",
    "None of the restaurant's attributes such as cuisines, service types, and venue types yielded any significant result in identifying restaurant's status. Price (general dining cost) did not matter whether the restaurant will remain open or not as indicated in Machine Learning Algorithm's feature importance. In conclusion, As common as this may sound, having long lifespan and positive sentiment scores were shown to be great predictors in identifying healthy stable restaurants as proven through data analysis and multiple machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
